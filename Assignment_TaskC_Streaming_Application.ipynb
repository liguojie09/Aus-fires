{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from json import JSONDecodeError\n",
    "\n",
    "import pymongo\n",
    "import time\n",
    "\n",
    "from bson import ObjectId\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import geohash2\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = '/usr/local/Cellar/python/3.7.1/bin/python3'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'ipython3'\n",
    "\n",
    "\n",
    "def insert_climate_to_db(client, climates):\n",
    "    \"\"\"\n",
    "    Insert the given list to mongodb\n",
    "    :param climates: A list of climate records\n",
    "    \"\"\"\n",
    "    if len(climates) == 0:\n",
    "        return\n",
    "    db = client.fit5148_assignment_db\n",
    "    db.climate_streaming.insert_many(climates)\n",
    "\n",
    "\n",
    "def insert_hotspot_to_db(client, hotspots):\n",
    "    \"\"\"\n",
    "    Insert the given list to mongodb\n",
    "    :param hotspots: A list of hotspot records\n",
    "    \"\"\"\n",
    "    if len(hotspots) == 0:\n",
    "        return\n",
    "\n",
    "    db = client.fit5148_assignment_db\n",
    "    for h in hotspots:\n",
    "        print(h)\n",
    "    db.hotspot_streaming.insert_many(hotspots)\n",
    "\n",
    "\n",
    "def is_joinable(record1, record2):\n",
    "    \"\"\"\n",
    "    Use geo-hash to determine whether two reords are joinable\n",
    "    :param record1: hotspot or climate\n",
    "    :param record2: hotspot or climate\n",
    "    :return: True if the precision of geo-hash is smaller than 5\n",
    "    \"\"\"\n",
    "    if 'latitude' not in record1 or 'latitude' not in record2:\n",
    "        raise Exception(\"record should have location information (latitude)\")\n",
    "    if 'longitude' not in record1 or 'longitude' not in record2:\n",
    "        raise Exception(\"record should have location information (longitude)\")\n",
    "    precision = 5\n",
    "    lat_1 = record1[\"latitude\"]\n",
    "    lat_2 = record2[\"latitude\"]\n",
    "    lng_1 = record1[\"longitude\"]\n",
    "    lng_2 = record2[\"longitude\"]\n",
    "    geo_hash_1 = geohash2.encode(lat_1, lng_1, precision)\n",
    "    geo_hash_2 = geohash2.encode(lat_2, lng_2, precision)\n",
    "    if geo_hash_1 == geo_hash_2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def handle_partition(iter):\n",
    "    \"\"\"\n",
    "    Perform operations on each batch\n",
    "    :param iter: A list of elements from the batch\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    climates = []\n",
    "    hotspots = []\n",
    "    ''' split climate and hotspot '''\n",
    "    for key, str_record in iter:\n",
    "        record = {}  # load data as a dictionary\n",
    "        try:\n",
    "            record = json.loads(str_record)\n",
    "        except JSONDecodeError:\n",
    "            print(\"Cannot deserialize data -> \" + str_record)\n",
    "            continue\n",
    "\n",
    "        if record['sender_id'] == \"climate\":\n",
    "            climates.append(record)\n",
    "        elif record['sender_id'] == \"AQUA\" or record['sender_id'] == \"TERRA\":\n",
    "            hotspots.append(record)\n",
    "        else:\n",
    "            raise Exception(\"Invalid sender id\")\n",
    "\n",
    "    ''' join two hotspot '''\n",
    "    if len(hotspots) == 2:\n",
    "        if is_joinable(hotspots[0], hotspots[1]):  # hotspot records are joinable\n",
    "            print(\"joinable hotspots\")\n",
    "            avg_surface_temperature = (hotspots[0]['surface_temperature_celcius']\n",
    "                                       + hotspots[1]['surface_temperature_celcius']) / 2\n",
    "            avg_confidence = (hotspots[0]['confidence'] + hotspots[1]['confidence']) / 2\n",
    "\n",
    "            ''' combine joinable hotspots '''\n",
    "            hotspot_to_store = hotspots[0]\n",
    "            hotspot_to_store['surface_temperature_celcius'] = avg_surface_temperature\n",
    "            hotspot_to_store['confidence'] = avg_confidence\n",
    "            hotspots = [hotspot_to_store]\n",
    "            print(hotspots)\n",
    "\n",
    "    ''' prepare the records that will be saved to the database '''\n",
    "    hotspots_to_save = []\n",
    "    for climate in climates:\n",
    "        climate['_id'] = ObjectId()\n",
    "        climate['created_time'] = datetime.datetime.strptime(climate['created_time'], '%Y-%m-%dT%H:%M:%S')\n",
    "        for hotspot in hotspots:\n",
    "            if is_joinable(climate, hotspot):  # bind climate and hotspot if joinable\n",
    "                print(\"joinable climate and hotspot\")\n",
    "                hotspot['climate_id'] = climate['_id']\n",
    "                hotspot['created_time'] = datetime.datetime.strptime(hotspot['created_time'], '%Y-%m-%dT%H:%M:%S')\n",
    "                hotspots_to_save.append(hotspot)\n",
    "\n",
    "\n",
    "    ''' save results to the database '''\n",
    "    client = MongoClient(\"localhost\", 27017)\n",
    "    # print(climates)\n",
    "    # print(hotspots)\n",
    "    insert_climate_to_db(client, climates)\n",
    "    insert_hotspot_to_db(client, hotspots_to_save)\n",
    "    client.close()\n",
    "\n",
    "\n",
    "n_secs = 10  # interval between each batch\n",
    "topic = \"temperature_analysis\"\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "consumer = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'week11-group',\n",
    "    'fetch.message.max.bytes': '15728640',\n",
    "    'auto.offset.reset': 'largest'},\n",
    "                                         valueDecoder=lambda x: x.decode('utf-8').replace(\"'\", \"\\\"\"),\n",
    "                                         # decode as dictionary\n",
    "                                         keyDecoder=lambda x: x.decode('utf-8'))  # decode as string\n",
    "\n",
    "consumer.foreachRDD(lambda rdd: rdd.foreachPartition(handle_partition))  # able to run concurrently\n",
    "\n",
    "ssc.start()\n",
    "\n",
    "time.sleep(600)  # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
